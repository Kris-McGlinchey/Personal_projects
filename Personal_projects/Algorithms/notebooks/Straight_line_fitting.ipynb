{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different ways of modelling straight line fitting\n",
    "\n",
    "This notebook is a simple record of the different ways that a straight line fitting can be carried out with varying degrees of complexity. We start with pre-made functions from numpy and statsmodels, building up to fully bayesian covariance models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import pymc4 as pm\n",
    "import arviz as az\n",
    "from statsmodels.regression.linear_model import OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fictional straight line\n",
    "n_val = 70\n",
    "x = np.arange(0, n_val, 1)\n",
    "y = 3 * x + 8 * 1 + stats.norm.rvs(0, 3, n_val)\n",
    "\n",
    "#%%\n",
    "b = np.ones(len(x))\n",
    "X = np.zeros((n_val, 2))\n",
    "\n",
    "X[:, 0] = x\n",
    "X[:, 1] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.97736315 8.60819055]\n"
     ]
    }
   ],
   "source": [
    "# polyfit automatically adds bias term so only include first column\n",
    "print(np.polyfit(X[:,0], y, deg = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.998</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>3.868e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 30 Sep 2020</td> <th>  Prob (F-statistic):</th> <td>1.95e-95</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:32:49</td>     <th>  Log-Likelihood:    </th> <td> -164.09</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    70</td>      <th>  AIC:               </th> <td>   332.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    68</td>      <th>  BIC:               </th> <td>   336.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    2.9774</td> <td>    0.015</td> <td>  196.667</td> <td> 0.000</td> <td>    2.947</td> <td>    3.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    8.6082</td> <td>    0.605</td> <td>   14.222</td> <td> 0.000</td> <td>    7.400</td> <td>    9.816</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.664</td> <th>  Durbin-Watson:     </th> <td>   1.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.435</td> <th>  Jarque-Bera (JB):  </th> <td>   1.219</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.026</td> <th>  Prob(JB):          </th> <td>   0.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.356</td> <th>  Cond. No.          </th> <td>    79.2</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.998\n",
       "Model:                            OLS   Adj. R-squared:                  0.998\n",
       "Method:                 Least Squares   F-statistic:                 3.868e+04\n",
       "Date:                Wed, 30 Sep 2020   Prob (F-statistic):           1.95e-95\n",
       "Time:                        15:32:49   Log-Likelihood:                -164.09\n",
       "No. Observations:                  70   AIC:                             332.2\n",
       "Df Residuals:                      68   BIC:                             336.7\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             2.9774      0.015    196.667      0.000       2.947       3.008\n",
       "const          8.6082      0.605     14.222      0.000       7.400       9.816\n",
       "==============================================================================\n",
       "Omnibus:                        1.664   Durbin-Watson:                   1.708\n",
       "Prob(Omnibus):                  0.435   Jarque-Bera (JB):                1.219\n",
       "Skew:                           0.026   Prob(JB):                        0.544\n",
       "Kurtosis:                       2.356   Cond. No.                         79.2\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statsmodels will provide more information of the fit, given p-values, CI etc\n",
    "model = OLS(y, X)\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.97736315 8.60819055]\n"
     ]
    }
   ],
   "source": [
    "weights = np.dot(np.linalg.inv(np.dot(X.T, X)), np.dot(X.T, y))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, n_val, n_iter):\n",
    "    \n",
    "    y = y.reshape((n_val, 1))\n",
    "    for _ in np.arange(n_iter):\n",
    "        y_hat = X.dot(theta)\n",
    "        error = X.T.dot(y_hat - y) / n_val\n",
    "        theta_new = theta - alpha * X.T.dot(y_hat - y) / n_val\n",
    "        theta = theta_new\n",
    "    return theta_new\n",
    "\n",
    "\n",
    "def gradient_descent_SGD(X, y, theta, alpha, n_val, n_iter):\n",
    "    \n",
    "    y = y.reshape((n_val, 1))\n",
    "    for _ in np.arange(n_iter):\n",
    "        for j in np.arange(n_val):\n",
    "            rand_int = np.random.randint(0, n_val)\n",
    "            X_i = X[rand_int, :].reshape(1, 2)\n",
    "            y_i = y[rand_int, :].reshape(1, 1)\n",
    "            y_hat = X_i.dot(theta)\n",
    "            error = X_i.T.dot(y_hat - y_i) / n_val\n",
    "            theta_new = theta - alpha * X_i.T.dot(y_hat - y_i) / n_val\n",
    "            theta = theta_new\n",
    "    return theta\n",
    "\n",
    "def gradient_descent_minibatch(X, y, theta, alpha, n_val, n_iter):\n",
    "    \n",
    "    y = y.reshape((n_val, 1))\n",
    "    for _ in np.arange(n_iter):\n",
    "        indices = np.random.permutation(n_val)\n",
    "        X = X[indices, :]\n",
    "        y = y[indices, :]\n",
    "        for j in np.arange(0, n_val, 5):\n",
    "            X_i = X[j:j+5, :]\n",
    "            y_i = y[j:j+5, :]\n",
    "            y_hat = X_i.dot(theta)\n",
    "            error = X_i.T.dot(y_hat - y_i) / n_val\n",
    "            theta_new = theta - alpha * X_i.T.dot(y_hat - y_i) / n_val\n",
    "            theta = theta_new\n",
    "    return theta\n",
    "\n",
    "#%%\n",
    "theta = np.array([[1], [1]]) \n",
    "theta_gd = gradient_descent(X, y, theta, 0.001, n_val, 200000)\n",
    "theta = np.array([[1], [1]])\n",
    "theta_sgd = gradient_descent_SGD(X, y, theta, 0.001, n_val, 20000)\n",
    "theta = np.array([[1], [1]])\n",
    "theta_minibatch = gradient_descent_minibatch(X, y, theta, 0.001, n_val, 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0017324 ]\n",
      " [7.74266205]]\n",
      "[[3.01208627]\n",
      " [7.70217484]]\n",
      "[[2.99983257]\n",
      " [7.70388884]]\n"
     ]
    }
   ],
   "source": [
    "print(theta_gd)\n",
    "print(theta_sgd)\n",
    "print(theta_minibatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# bayesian approach - grid search\n",
    "\n",
    "slope_grid = np.arange(1, 5, 0.1)\n",
    "intercept_grid = np.arange(5, 10, 0.1)\n",
    "total_grid = np.mgrid[1.:5.:0.1, 5.:10.:0.1].reshape(2, -1).T\n",
    "\n",
    "results = []\n",
    "for params in total_grid:\n",
    "    slope_prior = stats.norm.pdf(params[0], loc = 5, scale = 2)\n",
    "    intercept_prior = stats.norm.pdf(params[1], loc = 7, scale = 2)\n",
    "    results.append(np.sum(stats.norm.pdf(y, loc = params[0] * X[:, 0] + params[1] * X[:, 1], scale = 4)) * \\\n",
    "              slope_prior * intercept_prior)\n",
    "        \n",
    "results /= np.sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3. , 7.1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_grid[results.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "linear_fit/slope      3.006  0.017   2.973    3.039      0.000    0.000   \n",
      "linear_fit/intercept  7.529  0.690   6.220    8.793      0.014    0.010   \n",
      "linear_fit/std        3.043  0.262   2.584    3.547      0.004    0.003   \n",
      "\n",
      "                      ess_mean  ess_sd  ess_bulk  ess_tail  r_hat  \n",
      "linear_fit/slope        3013.0  3012.0    3015.0    3909.0   1.00  \n",
      "linear_fit/intercept    2313.0  2313.0    2311.0    2408.0   1.01  \n",
      "linear_fit/std          4414.0  4070.0    5006.0    4259.0   1.00  \n"
     ]
    }
   ],
   "source": [
    "# bayesian approach - HMC w/ pymc4\n",
    "\n",
    "@pm.model\n",
    "def linear_fit(data, obs):\n",
    "    slope_prior = yield pm.Normal(name = 'slope', loc = 4, scale = 1)\n",
    "    intercept_prior = yield pm.Normal(name = 'intercept', loc = 6, scale = 2)\n",
    "    scale_prior = yield pm.Uniform(name = 'std', low = 0, high = 10)\n",
    "    likelihood = yield pm.Normal(name = 'data', loc = data[:,0] * slope_prior + data[:, 1] * intercept_prior, scale = scale_prior, observed = obs)\n",
    "    return likelihood\n",
    "\n",
    "estimation = linear_fit(X, y)\n",
    "trace = pm.sample(estimation, num_samples = 1000)\n",
    "\n",
    "print(az.summary(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian approach w/ covariance- HMC w/ pymc4\n",
    "\n",
    "@pm.model\n",
    "def linear_fit(data, obs):\n",
    "    slope_prior = yield pm.Normal(name = 'slope', loc = 4, scale = 1)\n",
    "    intercept_prior = yield pm.Normal(name = 'intercept', loc = 6, scale = 2)\n",
    "    scale_prior = yield pm.Uniform(name = 'std', low = 0, high = 10)\n",
    "    likelihood = yield pm.Normal(name = 'data', loc = data[:,0] * slope_prior + data[:, 1] * intercept_prior, scale = scale_prior, observed = obs)\n",
    "    return likelihood\n",
    "\n",
    "estimation = linear_fit(X, y)\n",
    "trace = pm.sample(estimation, num_samples = 1000)\n",
    "\n",
    "print(az.summary(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "#plot results - matplotlib\n",
    "plt.plot(x, y)\n",
    "\n",
    "#plot results - seaborn\n",
    "import seaborn as sns\n",
    "sns.relplot(x = x, y = y, kind = 'scatter')\n",
    "\n",
    "#plot results - plotly\n",
    "from plotly.offline import plot\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "trace = go.Scatter(x = x, y = y)\n",
    "fig = go.Figure(data = [trace])\n",
    "plot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
